import { gestures } from './gestures.js'

// Config do video, largura, altura e o fps. Fps maximo em navegadores Ã© 60fps
const config = {
  video: { width: 640, height: 480, fps: 60 }
}

// Cor dos pontos que aparecem no video, de acordo com o nome de cada dedo
const landmarkColors = {
  thumb: 'red',
  index: 'blue',
  middle: 'yellow',
  ring: 'green',
  pinky: 'pink',
  wrist: 'white'
}

// Os gestos que irÃ£o aparecer na tela, caso bata com a implementaÃ§Ã£o da logica que esta em gesture, se esta fechado ou nÃ£o etc
const gestureStrings = {
  thumbs_up: 'ðŸ‘',
  victory: 'âœŒðŸ»',
  rock: 'âœŠï¸',
  paper: 'ðŸ–',
  scissors: 'âœŒï¸',
  dont: 'ðŸ™…â€â™‚ï¸'
}

const base = ['Horizontal ', 'Diagonal Up ']
const dont = {
  // MÃ£o esquerda pro lado direito
  left: [...base].map(i => i.concat('Right')),
  // MÃ£o direita pro lado esquerdo
  right: [...base].map(i => i.concat('Left'))
}

// Tensorflow usa as soluÃ§Ãµes do mediapipe ML(machine learning) por baixo dos panos para implementar o modelo handpose do tensorflow
async function createDetector() {
  return window.handPoseDetection.createDetector(
    // Aqui estamos utilizando o modelo handPoseDetection do tensorflow
    window.handPoseDetection.SupportedModels.MediaPipeHands,
    {
      runtime: 'mediapipe',
      modelType: 'full',
      // Quantas mÃ£os podem ser detectadas ao mesmo tempo
      maxHands: 2,
      solutionPath: `https://cdn.jsdelivr.net/npm/@mediapipe/hands@0.4.1646424915`
    }
  )
}

async function main() {
  // Tag video
  const video = document.querySelector('#pose-video')
  // Pegando o contexto do canvas, pra colocar os pontos na mÃ£o
  const canvas = document.querySelector('#pose-canvas')
  const ctx = canvas.getContext('2d')

  // A onde ira aparecer o resultado da detecÃ§Ã£o da mÃ£o com os gestos, seja uma img ou gif ou emoji etc
  const resultLayer = {
    right: document.querySelector('#pose-result-right'),
    left: document.querySelector('#pose-result-left')
  }
  // configure gesture estimator
  // add "âœŒðŸ»" and "ðŸ‘" as sample gestures and gestures.js
  // Os gestos que irÃ£o aparecer na tela(gestos conhecidos)
  const knownGestures = [
    // Esses dois gestos sÃ£o do proprio fingerpose, ja com a logica do ml do fp implementada
    fp.Gestures.VictoryGesture, // âœŒðŸ»
    fp.Gestures.ThumbsUpGesture, // ðŸ‘
    // Retorna um array com os gestos que foram importados do arquivo gestures.js com a devida logica de ml do fingerpose se esta fechando um dedo ou abrindo etc
    ...gestures // âœŠðŸ», ðŸ–, âœ‚ï¸, ðŸ™…â€â™‚ï¸
  ]

  // Aqui o GestureEstimator do fingerpose esta recebendo o array de gestos que foi importado do arquivo gestures.js e da propria lib fingerpose. Ou seja o metodo Estimador de gestos pega os gestos conhecidos
  const GE = new fp.GestureEstimator(knownGestures)

  // load handpose model.
  const detector = await createDetector()
  console.log('mediaPose model loaded')

  // Essa lista faz parte da logica para fazer o dont
  const pair = new Set() // Ã© uma lista que nÃ£o aceita repetiÃ§Ã£o

  // Verifica se tem as duas mÃ£os e se estÃ£o fazendo o mesmo gesto
  function checkGestureCombination(chosenHand, poseData) {
    console.log({ 'ChoseHand: ': chosenHand, 'Pose data: ': poseData })
    
    const addToPairIfCorrect = chosenHand => {
      // A gente pega o primeiro dedo(some), no caso o thumb(dedado) e ve pra que lado esta sendo apontado e se esta nas posiÃ§Ãµes horizontal ou Diagonal Up. Na posiÃ§Ã£o 2 do array finger(dedo) ele me fala tudo isso, ex: Diagonal Up Right.
      // No fim estamos vendo se a mÃ£o esquerda e direta estÃ£o de forma horizontal ou Diagonal Up
      const containsHand = poseData.some(finger => {
        console.log('finger ',finger)
        // Ele busca dentro de dont(objeto) em relaÃ§Ã£o a direÃ§Ã£o da mÃ£o(chosenHand), pois a direÃ§Ã£o Ã© a chave, se esta pra esquerda ou direta. Se levantar a mÃ£o esquerda entÃ£o ela esta apontando pra direita, com isso na chave left, ele retorna duas posiÃ§Ãµes dentro do array: ['Horizontal Right', 'Diagonal Up Right'] e depois compara se dentro do finger[2] tem algum deles
        console.log(dont[chosenHand])
        // ex de retorno: dont[chosenHand].includes(Horizontal Right) = true
        return dont[chosenHand].includes(finger[2])
      })

      if (!containsHand) return
      // Se estiver horizontal ou Diagonal Up, adiciona a direÃ§Ã£o da mÃ£o dentro de pair, left ou right.
      pair.add(chosenHand)
    }

    addToPairIfCorrect(chosenHand)
    // O tamanho do pair tem que ser dois, ou seja duas mÃ£os left e right
    if (pair.size !== 2) return
    // Se estiver com as duas mÃ£os ele coloca o emoji dont na tela
    resultLayer.left.innerText = resultLayer.right.innerText =
      gestureStrings.dont
    pair.clear()
  }

  // main estimation loop. Estimativa das mÃ£os para os gestoes em loop
  const estimateHands = async () => {
    // Limpa todas as informaÃ§Ãµes do canvas, toda vez que mexer a mÃ£o:

    // clear canvas overlay. O mÃ©todo CanvasRenderingContext2D.clearRect() da API Canvas 2D limpa todos os pixels de um retÃ¢ngulo definido na posiÃ§Ã£o (x, y) e tamanho (width (largura), height (altura)) para uma cor preta transparente, apagando algum conteÃºdo anterior. https://developer.mozilla.org/pt-BR/docs/Web/API/CanvasRenderingContext2D/clearRect
    ctx.clearRect(0, 0, config.video.width, config.video.height)
    // Limpa os resultados, no caso os emojis
    resultLayer.right.innerText = ''
    resultLayer.left.innerText = ''

    // Retorna um array das mÃ£os que foram detectadas e as coordenadas de cada ponto da mÃ£o, ou seja objetos com as coordenadas x e y e z dos dedos, da palma mÃ£o, o lado da mÃ£o etc.
    // handpose model estimativa de quantas mÃ£o estÃ£o aparecendo no video
    // get hand landmarks from video
    const hands = await detector.estimateHands(video, {
      // Ele inverte a camera pro handpose model entender qual Ã© a mÃ£o direita e esquerda corretamente
      flipHorizontal: true
    })

    // Manipulando o array de mÃ£os, no nosso caso 2 mÃ£os, ou seja manipulando cada uma delas a right e a left
    // Deixando claro que os keypoints sÃ£o as coordenadas das juntas de tal dedo ou da palma da mÃ£o
    for (const hand of hands) {
      // Manipulando as juntas(keypoints) da mÃ£o. A mÃ£o retorna as chaves dos pontos da mÃ£o, ou seja a propria palma da mÃ£o(wrist) possui um ponto e as juntas dos dedos tambem, o lado da mÃ£o(handedness) e score. Pra cada ponto ele retora o nome desse ponto, que sÃ£o as juntas da quele dedo ou a propria palma da mÃ£o.
      // console.log(hand)
      for (const keypoint of hand.keypoints) {
        // Pode ser a palma da mÃ£o ou ate mesmo a junta do dedo indicador etc. Pega todos os nomes das juntas
        const name = keypoint.name.split('_')[0].toString().toLowerCase()
        // Pra cada junta de tal dedo uma cor
        const color = landmarkColors[name]
        // Colocando os pontos na mÃ£o. Passando o canvas, os pontos da mÃ£o(as juntas), no caso a coordenada de cada junta pra cada ponto colorido ficar no video e as cores que Ã© pra cada junta de tal dedo Ã© uma cor, por exemplo pro dedo indicador Ã© o blue
        // console.log(color)
        drawPoint(ctx, keypoint.x, keypoint.y, 3, color)
      }

      // Aqui foi a manipulaÃ§Ã£o dos dados para o novo modelo de ML que estamos usando. Pegando novamente os pontos(as juntas) de cada mÃ£o  e retornado como um array. E dentro desse array Ã© retornado as 21(4 de cada dedo e 1 na palma da mÃ£o) juntas da nossa mÃ£o, que sÃ£o arrays com x,y e z. Ou seja as coordenadas de cada junta da mÃ£o
      const keypoints3D = hand.keypoints3D.map(keypoint => [
        keypoint.x,
        keypoint.y,
        keypoint.z
      ])

      // console.log(keypoints3D)

      // previsÃµes de estimativa de acordo com os keypoint pro fingerpose para os gestos. Nesse caso ele retorna dois objetos um poseData e um gesture. O pose data retorna os dedos e se ele esta no curl, horizontal de acordo com os keypoints de cada juntas dos dedos e da palma da mÃ£o. E o gesture Ã© caso se ele fizer o posiÃ§Ã£o correta pra tal gesto colocado la em cima em const GE = new fp.GestureEstimator(knownGestures) que foi definido a logica de cada gesto em gesture.js ou da propria lib fingerpose por exemplo fp.Gestures.VictoryGesture que ja retorna a implementaÃ§Ã£o de dois gestos e que foi colocado dentro de um array com o nome knownGestures e passado pro fp. Ou seja passando os keypoints da mÃ£o pro fp e o knownGestures, ele vai saber se esta nocurl, horizontal etc e se bater com a logica de tal gesto colocado aqui GestureEstimator, ele aparece na tela.
      // Resumindo: de acordo com as coordenadas das juntas das mÃ£os o fp vai saber me falar se o dedo esta full curl, no curl, pra qual direÃ§Ã£o esta apontando aquele dedo. com isso, retornando a poseData, que nada mais Ã© a posiÃ§Ã£o do dedo e o gesture, o gesto formado pelos dedos.
      // O 9 Ã© o score do dedo, ou seja se o score for maior que 9, ele vai retornar o gesto. Ou seja, score seria por exemplo pra ele aceitar eu fazer o papel com um dedo meu que quase nÃ£o abre direito, Ã© meio torto etc.
      const predictions = GE.estimate(keypoints3D, 9)
      // console.log(predictions)

      // Aqui somente o debug ao lado do video para ver como que a gente pode criar os nossos gestos(gestures), especificamente o dont
      if (!predictions.gestures.length) {
        updateDebugInfo(predictions.poseData, 'left')
      }

      // Se nas prediÃ§Ãµes em gesture tiver alguma coisa, Ã© porque os keypoints dos dedos bateu com alguma logica dos gestos colocado em gesture no metodo GestureDescription, com isso pega o nome desse gesto que foi colocado la e compara com os nomes dentro do objeto gestureStrings, se bater, ele coloca a string do gesto na tela
      if (predictions.gestures.length > 0) {
        // O result, retorna um objeto de duas posiÃ§Ãµes que tem dentro de gesture, que Ã© o score e o nome do gesto
        const result = predictions.gestures.reduce((p, c) =>
          p.score > c.score ? p : c
        )
        // console.log(result)

        // Found retorna o valor(emoji do gesto) da chave passada result.name pro objeto gestureStrings.
        const found = gestureStrings[result.name]
        // find gesture with highest match score. handedness o lado da mÃ£o aparecendo no video e o predictions.poseData Ã© cada info dos dedos se esta nocurl, horizontal etc, passado pro debuginfo
        const chosenHand = hand.handedness.toLowerCase()
        updateDebugInfo(predictions.poseData, chosenHand)

        // Se o found for diferente da chave dont do objeto gestureStrings, entÃ£o continue o loop normalmente
        if (found !== gestureStrings.dont) {
          // Resultado da mÃ£o, seja ela esquerda ou direta
          resultLayer[chosenHand].innerText = found
          continue
        }
        // Um metodo de combinaÃ§Ã£o de duas mÃ£os pra fazer o dont
        checkGestureCombination(chosenHand, predictions.poseData)
      }
    }
    // ...and so on
    setTimeout(() => {
      estimateHands()
    }, 1000 / config.video.fps)
  }

  estimateHands()
  console.log('Starting predictions')
}

// Para iniciar a camera
async function initCamera(width, height, fps) {
  // restriÃ§Ãµes do video, ou melhor dizendo, um objeto de config do video
  // https://developer.mozilla.org/en-US/docs/Web/API/MediaTrackConstraints/facingMode
  const constraints = {
    audio: false,
    video: {
      // A propriedade faceMode do dicionÃ¡rio MediaTrackConstraints Ã© uma ConstrainDOMString que descreve as restriÃ§Ãµes solicitadas ou obrigatÃ³rias colocadas sobre o valor da propriedade constrainable facesMode.
      facingMode: 'user',
      // os valores sÃ£o passados dinamicamente por parametro, supostamente vindo do objeto config
      width: width,
      height: height,
      frameRate: { max: fps }
    }
  }

  // tag video e seta os valores
  const video = document.querySelector('#pose-video')
  video.width = width
  video.height = height

  // get video stream
  // O mÃ©todo MediaDevices.getUserMedia() solicita ao usuÃ¡rio permissÃ£o para usar uma entrada de mÃ­dia que produz um MediaStream com faixas contendo os tipos de mÃ­dia solicitados. https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia
  const stream = await navigator.mediaDevices.getUserMedia(constraints)
  // A propriedade srcObject da interface HTMLMediaElement define ou retorna o objeto que serve como fonte da mÃ­dia associada ao HTMLMediaElement. https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/srcObject
  video.srcObject = stream

  return new Promise(resolve => {
    // O evento loadmetadata Ã© acionado quando os metadados sÃ£o carregados. Um event
    // https://developer.mozilla.org/pt-BR/docs/Web/JavaScript/Reference/Global_Objects/Promise/resolve
    // https://www.freecodecamp.org/portuguese/news/tutorial-de-promises-do-javascript-resolve-reject-e-encadeamento-em-js-e-na-es6/
    video.onloadedmetadata = () => {
      resolve(video)
    }
  })
}

// Pra desenhar Pontos da mÃ£o. Necessariamente nÃ£o precisa disso, mas Ã© uma forma de debugar
function drawPoint(ctx, x, y, r, color) {
  // Ctx Ã© o contexto do canvas, pra colocar os pontos na mÃ£o

  // O mÃ©todo CanvasRenderingContext2D.beginPath() da API Canvas 2D inicia um novo caminho (path), esvaziando a lista de sub-caminhos (sub-paths). Use esse mÃ©todo quando vocÃª quiser criar um novo path. https://developer.mozilla.org/pt-BR/docs/Web/API/CanvasRenderingContext2D/beginPath
  ctx.beginPath()
  // O mÃ©todo CanvasRenderingContext2D.arc() da API Canvas 2D adiciona um arco circular para o atual sub-caminhoa (sub-path). https://developer.mozilla.org/pt-BR/docs/Web/API/CanvasRenderingContext2D/arc
  ctx.arc(x, y, r, 0, 2 * Math.PI)
  // A propriedade CanvasRenderingContext2D.fillStyle da API do Canvas 2D especifica a cor ou o estilo para usar regiÃµes internas. O valor inicial Ã© #000 (preto).
  ctx.fillStyle = color
  // O mÃ©todo CanvasRenderingContext2D.fill() da API Canvas 2D preenche um dado path ou o path atual com o estilo atual de preenchimento usando uma regra de controle diferente de zero, ou uma regra par-Ã­mpar.
  ctx.fill()
}

// Debug ao lado do video
function updateDebugInfo(data, hand) {
  // Pega o id da tabela de resumo de cada mÃ£o, passado o lado da mÃ£o como parametro, ex: summary-left
  const summaryTable = `#summary-${hand}`
  // Coloca o valor de cada dedo na tabela de resumo, cada array de cada lado da mÃ£o(data, ou seja o predictions.poseData) possui 5 posiÃ§Ãµes/dedos e cada um desses dedos(fingerIdx) me retorna um array de tres posiÃ§Ãµes de objetos, a primeira[0] o dedo, a segunda[1] se esta aberto ou nÃ£o(curl) e a terceira[2] direÃ§Ã£o dele.ex: 4:(3) ['Pinky', 'No Curl', 'Vertical Up']
  for (let fingerIdx in data) {
    // Se o dedo esta aberto ou fechado(Curl)
    document.querySelector(`${summaryTable} span#curl-${fingerIdx}`).innerHTML =
      data[fingerIdx][1]
    // Qual direÃ§Ã£o esta indo o dedo(Direction)
    document.querySelector(`${summaryTable} span#dir-${fingerIdx}`).innerHTML =
      data[fingerIdx][2]
  }
}

// Coloca o video na tela
// O evento DOMContentLoaded Ã© acionado quando todo o HTML foi completamente carregado e analisado, sem aguardar pelo CSS, imagens, e subframes para encerrar o carregamento. Um evento muito diferente - load (en-US) - deve ser usado apenas para detectar uma pÃ¡gina completamente carregada. Ã‰ um engano comum as pessoas usarem load (en-US) quando DOMContentLoaded seria muito mais apropriado. https://developer.mozilla.org/pt-BR/docs/Web/API/Window/DOMContentLoaded_event ou https://developer.mozilla.org/en-US/docs/Web/API/Window/DOMContentLoaded_event
window.addEventListener('DOMContentLoaded', () => {
  // Chama esse metodo pra iniciar a camera
  // .then ou poderia ser um await, porque o initCamera retorna uma promise de evento da tag video que retorna quando os metadados sÃ£o carregados da tag video e dessa forma ele consegue dar play no que Ã© retornado na promise, iniciando posteriormente o metodo main()
  // Como valores pros parametros de configuraÃ§Ã£o do video a gente passa o objeto config
  initCamera(config.video.width, config.video.height, config.video.fps).then(
    video => {
      video.play()
      video.addEventListener('loadeddata', event => {
        console.log('Camera is ready')
        main()
      })
    }
  )

  // Inicia o canvas, para aparecer os pontinhos
  const canvas = document.querySelector('#pose-canvas')
  canvas.width = config.video.width
  canvas.height = config.video.height
  console.log('Canvas initialized')
})
